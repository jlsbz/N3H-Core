nohup: ignoring input
PyTorch Version:  1.6.0
Torchvision Version:  0.7.0
==> Preparing data..
=> creating model 'res18_image'  pretrained is  True
    Total params: 11.69M
idx: [1, 7, 10, 13, 16, 20, 23, 26, 29, 32, 36, 39, 42, 45, 48, 52, 55, 58, 61, 64, 67]
[[8, 8, 0], [4, 4, 0.5], [4, 4, 0.5], [4, 4, 0.5], [4, 4, 0.5], [4, 4, 0.5], [4, 4, 0.66], [4, 4, 0.13], [4, 4, 0.66], [4, 4, 0.66], [4, 4, 0.94], [4, 4, 0.75], [4, 4, 0.13], [4, 4, 0.75], [4, 4, 0.75], [4, 4, 0.72], [4, 4, 0.85], [4, 4, 0.46], [4, 4, 0.85], [4, 4, 0.85], [8, 8, 0.28]]
==> start calibrate
==> end calibrate

test_acc: 74.024000
Epoch: [1 | 40] LR: 0.000500

train_acc: 65.627588 top1: 67.424000 top5: 87.856000
Epoch: [2 | 40] LR: 0.000499

train_acc: 66.535042 top1: 67.860000 top5: 88.046000
Epoch: [3 | 40] LR: 0.000497

train_acc: 66.892607 top1: 67.972000 top5: 88.214000
Epoch: [4 | 40] LR: 0.000493

train_acc: 67.006253 top1: 68.086000 top5: 88.194000
Epoch: [5 | 40] LR: 0.000488

train_acc: 67.235029 top1: 68.244000 top5: 88.418000
Epoch: [6 | 40] LR: 0.000481

train_acc: 67.342587 top1: 68.102000 top5: 88.330000
Epoch: [7 | 40] LR: 0.000473

train_acc: 67.365613 top1: 68.320000 top5: 88.334000
Epoch: [8 | 40] LR: 0.000463

train_acc: 67.442808 top1: 68.290000 top5: 88.340000
Epoch: [9 | 40] LR: 0.000452

train_acc: 67.542717 top1: 68.450000 top5: 88.200000
Epoch: [10 | 40] LR: 0.000440

train_acc: 67.616244 top1: 68.292000 top5: 88.356000
Epoch: [11 | 40] LR: 0.000427

train_acc: 67.656910 top1: 68.416000 top5: 88.330000
Epoch: [12 | 40] LR: 0.000412

train_acc: 67.733168 top1: 68.206000 top5: 88.404000
Epoch: [13 | 40] LR: 0.000397

train_acc: 67.808334 top1: 68.334000 top5: 88.442000
Epoch: [14 | 40] LR: 0.000381

train_acc: 67.853761 top1: 68.406000 top5: 88.590000
Epoch: [15 | 40] LR: 0.000363

train_acc: 67.880300 top1: 68.400000 top5: 88.528000
Epoch: [16 | 40] LR: 0.000346

train_acc: 68.003625 top1: 68.564000 top5: 88.496000
Epoch: [17 | 40] LR: 0.000327

train_acc: 68.071376 top1: 68.540000 top5: 88.546000
Epoch: [18 | 40] LR: 0.000308

train_acc: 68.137097 top1: 68.686000 top5: 88.552000
Epoch: [19 | 40] LR: 0.000289

train_acc: 68.180651 top1: 68.642000 top5: 88.610000
Epoch: [20 | 40] LR: 0.000270

train_acc: 68.245123 top1: 68.760000 top5: 88.576000
Epoch: [21 | 40] LR: 0.000250

train_acc: 68.347452 top1: 68.820000 top5: 88.632000
Epoch: [22 | 40] LR: 0.000230

train_acc: 68.368917 top1: 68.952000 top5: 88.680000
Epoch: [23 | 40] LR: 0.000211

train_acc: 68.426911 top1: 68.838000 top5: 88.700000
Epoch: [24 | 40] LR: 0.000192

train_acc: 68.559446 top1: 68.848000 top5: 88.650000
Epoch: [25 | 40] LR: 0.000173

train_acc: 68.585126 top1: 69.092000 top5: 88.734000
Epoch: [26 | 40] LR: 0.000154

train_acc: 68.623450 top1: 69.032000 top5: 88.692000
Epoch: [27 | 40] LR: 0.000137

train_acc: 68.681913 top1: 68.888000 top5: 88.744000
Epoch: [28 | 40] LR: 0.000119

train_acc: 68.705641 top1: 69.088000 top5: 88.820000
Epoch: [29 | 40] LR: 0.000103

train_acc: 68.818741 top1: 69.050000 top5: 88.882000
Epoch: [30 | 40] LR: 0.000088

train_acc: 68.857534 top1: 69.002000 top5: 88.790000
Epoch: [31 | 40] LR: 0.000073

train_acc: 68.912015 top1: 69.094000 top5: 88.792000
Epoch: [32 | 40] LR: 0.000060

train_acc: 68.960799 top1: 69.192000 top5: 88.786000
Epoch: [33 | 40] LR: 0.000048

train_acc: 69.008490 top1: 69.152000 top5: 88.918000
Epoch: [34 | 40] LR: 0.000037

train_acc: 69.111521 top1: 69.154000 top5: 88.846000
Epoch: [35 | 40] LR: 0.000027

train_acc: 69.134079 top1: 69.252000 top5: 88.876000
Epoch: [36 | 40] LR: 0.000019

train_acc: 69.176384 top1: 69.284000 top5: 89.004000
Epoch: [37 | 40] LR: 0.000012

train_acc: 69.218064 top1: 69.280000 top5: 88.944000
Epoch: [38 | 40] LR: 0.000007

train_acc: 69.271297 top1: 69.194000 top5: 88.896000
Epoch: [39 | 40] LR: 0.000003

train_acc: 69.292762 top1: 69.374000 top5: 88.950000
Epoch: [40 | 40] LR: 0.000001

train_acc: 69.355751 top1: 69.340000 top5: 88.912000
Best acc:
69.374
