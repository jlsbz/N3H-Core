nohup: ignoring input
PyTorch Version:  1.6.0
Torchvision Version:  0.7.0
==> Preparing data..
=> creating model 'res18_image'  pretrained is  True
    Total params: 11.69M
idx: [1, 7, 10, 13, 16, 20, 23, 26, 29, 32, 36, 39, 42, 45, 48, 52, 55, 58, 61, 64, 67]
[[8, -1], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [8, 8]]
==> start calibrate
==> end calibrate

test_acc: 20.498000
Epoch: [1 | 100] LR: 0.001000

train_acc: 64.776099 top1: 67.152000 top5: 87.652000
Epoch: [2 | 100] LR: 0.001000

train_acc: 65.893752 top1: 67.606000 top5: 87.900000
Epoch: [3 | 100] LR: 0.000999

train_acc: 66.172326 top1: 67.630000 top5: 87.890000
Epoch: [4 | 100] LR: 0.000998

train_acc: 66.384788 top1: 67.778000 top5: 87.998000
Epoch: [5 | 100] LR: 0.000996

train_acc: 66.629487 top1: 68.070000 top5: 88.138000
Epoch: [6 | 100] LR: 0.000994

train_acc: 66.649000 top1: 67.902000 top5: 88.188000
Epoch: [7 | 100] LR: 0.000991

train_acc: 66.799644 top1: 67.910000 top5: 88.166000
Epoch: [8 | 100] LR: 0.000988

train_acc: 66.850379 top1: 67.998000 top5: 88.270000
Epoch: [9 | 100] LR: 0.000984

train_acc: 66.933819 top1: 67.972000 top5: 88.148000
Epoch: [10 | 100] LR: 0.000980

train_acc: 67.043172 top1: 68.172000 top5: 88.280000
Epoch: [11 | 100] LR: 0.000976

train_acc: 67.109128 top1: 67.984000 top5: 88.410000
Epoch: [12 | 100] LR: 0.000970

train_acc: 67.111469 top1: 67.844000 top5: 88.286000
Epoch: [13 | 100] LR: 0.000965

train_acc: 67.238541 top1: 68.186000 top5: 88.334000
Epoch: [14 | 100] LR: 0.000959

train_acc: 67.243771 top1: 68.170000 top5: 88.306000
Epoch: [15 | 100] LR: 0.000952

train_acc: 67.314331 top1: 68.502000 top5: 88.588000
Epoch: [16 | 100] LR: 0.000946

train_acc: 67.485035 top1: 68.100000 top5: 88.258000
Epoch: [17 | 100] LR: 0.000938

train_acc: 67.521564 top1: 68.108000 top5: 88.320000
Epoch: [18 | 100] LR: 0.000930

train_acc: 67.488469 top1: 68.202000 top5: 88.508000
Epoch: [19 | 100] LR: 0.000922

train_acc: 67.628732 top1: 68.448000 top5: 88.452000
Epoch: [20 | 100] LR: 0.000914

train_acc: 67.676735 top1: 68.458000 top5: 88.472000
Epoch: [21 | 100] LR: 0.000905

train_acc: 67.725051 top1: 68.280000 top5: 88.346000
Epoch: [22 | 100] LR: 0.000895

train_acc: 67.809817 top1: 68.414000 top5: 88.452000
Epoch: [23 | 100] LR: 0.000885

train_acc: 67.846502 top1: 68.542000 top5: 88.332000
Epoch: [24 | 100] LR: 0.000875

train_acc: 67.852200 top1: 68.464000 top5: 88.452000
Epoch: [25 | 100] LR: 0.000864

train_acc: 67.907228 top1: 68.498000 top5: 88.504000
Epoch: [26 | 100] LR: 0.000854

train_acc: 68.000347 top1: 68.490000 top5: 88.366000
Epoch: [27 | 100] LR: 0.000842

train_acc: 68.052330 top1: 68.720000 top5: 88.620000
Epoch: [28 | 100] LR: 0.000831

train_acc: 68.087767 top1: 68.500000 top5: 88.532000
Epoch: [29 | 100] LR: 0.000819

train_acc: 68.186973 top1: 68.674000 top5: 88.614000
Epoch: [30 | 100] LR: 0.000806

train_acc: 68.264169 top1: 68.748000 top5: 88.586000
Epoch: [31 | 100] LR: 0.000794

train_acc: 68.235445 top1: 68.782000 top5: 88.564000
Epoch: [32 | 100] LR: 0.000781

train_acc: 68.319196 top1: 68.822000 top5: 88.570000
Epoch: [33 | 100] LR: 0.000768

train_acc: 68.356116 top1: 68.694000 top5: 88.616000
Epoch: [34 | 100] LR: 0.000755

train_acc: 68.411300 top1: 68.600000 top5: 88.662000
Epoch: [35 | 100] LR: 0.000741

train_acc: 68.446034 top1: 68.700000 top5: 88.620000
Epoch: [36 | 100] LR: 0.000727

train_acc: 68.529864 top1: 68.712000 top5: 88.626000
Epoch: [37 | 100] LR: 0.000713

train_acc: 68.541260 top1: 68.866000 top5: 88.674000
Epoch: [38 | 100] LR: 0.000699

train_acc: 68.625714 top1: 68.782000 top5: 88.656000
Epoch: [39 | 100] LR: 0.000684

train_acc: 68.717349 top1: 68.970000 top5: 88.698000
Epoch: [40 | 100] LR: 0.000669

train_acc: 68.728121 top1: 69.016000 top5: 88.728000
Epoch: [41 | 100] LR: 0.000655

train_acc: 68.796808 top1: 68.840000 top5: 88.784000
Epoch: [42 | 100] LR: 0.000639

train_acc: 68.843172 top1: 69.040000 top5: 88.726000
Epoch: [43 | 100] LR: 0.000624

train_acc: 68.898668 top1: 68.948000 top5: 88.574000
Epoch: [44 | 100] LR: 0.000609

train_acc: 68.976332 top1: 68.878000 top5: 88.608000
Epoch: [45 | 100] LR: 0.000594

train_acc: 68.979142 top1: 68.890000 top5: 88.540000
Epoch: [46 | 100] LR: 0.000578

train_acc: 69.068123 top1: 69.174000 top5: 88.810000
Epoch: [47 | 100] LR: 0.000563

train_acc: 69.084280 top1: 69.220000 top5: 88.872000
Epoch: [48 | 100] LR: 0.000547

train_acc: 69.199566 top1: 69.012000 top5: 88.790000
Epoch: [49 | 100] LR: 0.000531

train_acc: 69.181067 top1: 69.182000 top5: 88.886000
Epoch: [50 | 100] LR: 0.000516

train_acc: 69.283708 top1: 69.098000 top5: 88.814000
Epoch: [51 | 100] LR: 0.000500

train_acc: 69.272234 top1: 69.368000 top5: 88.832000
Epoch: [52 | 100] LR: 0.000484

train_acc: 69.359420 top1: 69.220000 top5: 88.790000
Epoch: [53 | 100] LR: 0.000469

train_acc: 69.428966 top1: 69.020000 top5: 88.996000
Epoch: [54 | 100] LR: 0.000453

train_acc: 69.557677 top1: 69.282000 top5: 88.964000
Epoch: [55 | 100] LR: 0.000437

train_acc: 69.506708 top1: 69.188000 top5: 88.952000
Epoch: [56 | 100] LR: 0.000422

train_acc: 69.611066 top1: 69.396000 top5: 88.982000
Epoch: [57 | 100] LR: 0.000406

train_acc: 69.631281 top1: 69.356000 top5: 88.942000
Epoch: [58 | 100] LR: 0.000391

train_acc: 69.688417 top1: 69.314000 top5: 88.904000
Epoch: [59 | 100] LR: 0.000376

train_acc: 69.748284 top1: 69.390000 top5: 88.990000
Epoch: [60 | 100] LR: 0.000361

train_acc: 69.809244 top1: 69.250000 top5: 88.760000
Epoch: [61 | 100] LR: 0.000345

train_acc: 69.845930 top1: 69.548000 top5: 89.056000
Epoch: [62 | 100] LR: 0.000331

train_acc: 69.903455 top1: 69.416000 top5: 88.958000
Epoch: [63 | 100] LR: 0.000316

train_acc: 69.981197 top1: 69.340000 top5: 89.016000
Epoch: [64 | 100] LR: 0.000301

train_acc: 69.991812 top1: 69.556000 top5: 88.966000
Epoch: [65 | 100] LR: 0.000287

train_acc: 70.023970 top1: 69.608000 top5: 89.096000
Epoch: [66 | 100] LR: 0.000273

train_acc: 70.144095 top1: 69.446000 top5: 88.984000
Epoch: [67 | 100] LR: 0.000259

train_acc: 70.203806 top1: 69.662000 top5: 89.144000
Epoch: [68 | 100] LR: 0.000245

train_acc: 70.258444 top1: 69.578000 top5: 89.138000
Epoch: [69 | 100] LR: 0.000232

train_acc: 70.166028 top1: 69.630000 top5: 89.166000
Epoch: [70 | 100] LR: 0.000219

train_acc: 70.276787 top1: 69.652000 top5: 89.186000
Epoch: [71 | 100] LR: 0.000206

train_acc: 70.394648 top1: 69.762000 top5: 89.232000
Epoch: [72 | 100] LR: 0.000194

train_acc: 70.399253 top1: 69.606000 top5: 89.176000
Epoch: [73 | 100] LR: 0.000181

train_acc: 70.465521 top1: 69.748000 top5: 89.076000
Epoch: [74 | 100] LR: 0.000169

train_acc: 70.520159 top1: 69.716000 top5: 89.242000
Epoch: [75 | 100] LR: 0.000158

train_acc: 70.561995 top1: 69.722000 top5: 89.108000
Epoch: [76 | 100] LR: 0.000146

train_acc: 70.534208 top1: 69.748000 top5: 89.130000
Epoch: [77 | 100] LR: 0.000136

train_acc: 70.532257 top1: 69.782000 top5: 89.126000
Epoch: [78 | 100] LR: 0.000125

train_acc: 70.664246 top1: 69.874000 top5: 89.128000
Epoch: [79 | 100] LR: 0.000115

train_acc: 70.695858 top1: 69.828000 top5: 89.254000
Epoch: [80 | 100] LR: 0.000105

train_acc: 70.687116 top1: 69.794000 top5: 89.270000
Epoch: [81 | 100] LR: 0.000095

train_acc: 70.731919 top1: 69.916000 top5: 89.266000
Epoch: [82 | 100] LR: 0.000086

train_acc: 70.863595 top1: 69.910000 top5: 89.208000
Epoch: [83 | 100] LR: 0.000078

train_acc: 70.881470 top1: 69.928000 top5: 89.224000
Epoch: [84 | 100] LR: 0.000070

train_acc: 70.850092 top1: 69.894000 top5: 89.316000
Epoch: [85 | 100] LR: 0.000062

train_acc: 70.901686 top1: 69.866000 top5: 89.314000
Epoch: [86 | 100] LR: 0.000054

train_acc: 70.969280 top1: 69.958000 top5: 89.292000
Epoch: [87 | 100] LR: 0.000048

train_acc: 70.980364 top1: 70.022000 top5: 89.400000
Epoch: [88 | 100] LR: 0.000041

train_acc: 71.052798 top1: 70.052000 top5: 89.240000
Epoch: [89 | 100] LR: 0.000035

train_acc: 71.040231 top1: 69.888000 top5: 89.242000
Epoch: [90 | 100] LR: 0.000030

train_acc: 71.051003 top1: 69.952000 top5: 89.274000
Epoch: [91 | 100] LR: 0.000024

train_acc: 71.096976 top1: 70.018000 top5: 89.244000
Epoch: [92 | 100] LR: 0.000020

train_acc: 71.193997 top1: 70.126000 top5: 89.360000
Epoch: [93 | 100] LR: 0.000016

train_acc: 71.113992 top1: 70.028000 top5: 89.420000
Epoch: [94 | 100] LR: 0.000012

train_acc: 71.169488 top1: 70.090000 top5: 89.394000
Epoch: [95 | 100] LR: 0.000009

train_acc: 71.218428 top1: 70.108000 top5: 89.410000
Epoch: [96 | 100] LR: 0.000006

train_acc: 71.205783 top1: 70.092000 top5: 89.334000
Epoch: [97 | 100] LR: 0.000004

train_acc: 71.227482 top1: 70.156000 top5: 89.340000
Epoch: [98 | 100] LR: 0.000002

train_acc: 71.258001 top1: 70.176000 top5: 89.358000
Epoch: [99 | 100] LR: 0.000001

train_acc: 71.236224 top1: 70.174000 top5: 89.374000
Epoch: [100 | 100] LR: 0.000000

train_acc: 71.288911 top1: 70.252000 top5: 89.404000
Best acc:
70.25/89.40
